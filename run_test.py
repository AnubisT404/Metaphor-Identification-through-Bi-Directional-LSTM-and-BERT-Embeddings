# -*- coding: utf-8 -*-
"""run_test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Uk573vobdSVlmEqvlgLOUdX9nECDCX8h
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import re


import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from gensim.models import Word2Vec


from collections import Counter
import string
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

import spacy


import xgboost as xgb
from hyperopt import fmin, hp, tpe


from sklearn.metrics import accuracy_score, classification_report
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report


import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Masking
from keras.layers import LSTM, Dense, Masking, SimpleRNN, StackedRNNCells, GRU, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.optimizers import Adam # - Works
from tensorflow.keras.models import load_model



from transformers import BertTokenizer, BertModel
from keras.models import Sequential
from keras.layers import SimpleRNN, Dense
import torch

model_LSTM = load_model('best_model.h5')

# Load pre-trained BERT model and tokenizer
model_name = 'bert-base-uncased'
tokenizer = BertTokenizer.from_pretrained(model_name)
bert_model = BertModel.from_pretrained(model_name)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(device)
bert_model.to(device)

df_test = pd.read_csv('test.csv')

df_test['label_boolean'] = df_test['label_boolean'].astype(int)
nlp = spacy.load('en_core_web_sm')
def spacy_lemmatize(text):
    text=text.lower()
    text = re.sub(r'\?+', '?', text)
    text = re.sub(r'\!+', '!', text)
    text = re.sub(r'\/+', '', text)
    text = re.sub(r'`', '', text)
    text = re.sub(r'-lrb-', '-', text)
    text = re.sub(r'-rrb-', '-', text)
    text = re.sub(r'\-+', '-', text)
    text = re.sub(r"I've", "i have", text)
    text = re.sub(r"i've", "i have", text)
    text = re.sub(r"I'm", "i am", text)
    text = re.sub(r"i'm", "i am", text)
    text = re.sub(r" - ", " . ", text)
    text = re.sub(r"i 'm", "i am", text)
    text = re.sub(r"i 've", "i have", text)
    doc = nlp(text)
    lemmatized_tokens = [token.lemma_ for token in doc]
    lemmatized_text = ' '.join(lemmatized_tokens)
    lemmatized_text = re.sub(r"'s\b", "s", lemmatized_text)
    return lemmatized_text
df_test['lemmatized_text'] = df_test['text'].apply(spacy_lemmatize)
important_sentences = []

for index, row in df_test.iterrows():
    text = ""  # Initialize an empty string

    if row['metaphorID'] == 0:
        keyword_search = 'road'
    elif row['metaphorID'] == 1:
        keyword_search = 'candle'
    elif row['metaphorID'] == 2:
        keyword_search = 'light'
    elif row['metaphorID'] == 3:
        keyword_search = 'spice'
    elif row['metaphorID'] == 4:
        keyword_search = 'ride'
    elif row['metaphorID'] == 5:
        keyword_search = 'train'
    elif row['metaphorID'] == 6:
        keyword_search = 'boat'

    for j in row['lemmatized_text'].split('.'):



        if keyword_search in j:
            if (' - ' not in j) and (' , ' not in j) and (' ; ' not in j) and (' ? ' not in j) and (' ! ' not in j):
                if keyword_search in j:
                        text = j
                        break

            elif ' , ' in j:
                for k in j.split(','):
                    if keyword_search in k:
                        text = k
                        break
            elif ' ; ' in j:
                for k in j.split(';'):
                    if keyword_search in k:
                        text = k
                        break
            elif ' ? ' in j:
                for k in j.split(' ? '):
                    if keyword_search in k:
                        text = k
                        break
            elif ' ! ' in j:
                for k in j.split(' ! '):
                    if keyword_search in k:
                        text = k
                        break
            elif ' - ' in j:
                for k in j.split(' - '):
                    if keyword_search in k:
                        text = k
                        break

    if text:
        important_sentences.append(text)
df_important_sentence = pd.DataFrame({'important_sentences': important_sentences})
df_test['important_sentences']=df_important_sentence['important_sentences']
test_xvalues=df_test['important_sentences'].to_list()
test_labels=df_test['label_boolean'].to_list()

tester_embeddings=[]
for sentence in test_xvalues:
    tokens = tokenizer(sentence, return_tensors='pt', padding=True, truncation=True)
    with torch.no_grad():
        for key in tokens:
            tokens[key] = tokens[key].to(device)
        outputs = bert_model(**tokens)
    cls_embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()
    tester_embeddings.append(cls_embedding)

# Convert the list of embeddings to a numpy array
tester_embeddings = np.array(tester_embeddings)
y_Pred=np.round(model_LSTM.predict(tester_embeddings))
y_Pred=y_Pred.flatten()

from sklearn.metrics import classification_report
# Generate classification report
report = classification_report( test_labels,y_Pred)

# Print the classification report
print("Classification Report:\n", report)

